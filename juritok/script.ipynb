{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sentencepiece as spm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    ID texte            ID article  Nature N° article  \\\n",
      "0       JORFTEXT000048734585                   NaN       0        NaN   \n",
      "1       JORFTEXT000048734585  JORFVERS000048734585       0        NaN   \n",
      "2       JORFTEXT000048734585  JORFARTI000048734586       1          1   \n",
      "3       JORFTEXT000048734585  JORFARTI000048734586       1          1   \n",
      "4       JORFTEXT000048734585  JORFARTI000048734586       1          1   \n",
      "...                      ...                   ...     ...        ...   \n",
      "454296  JORFTEXT000046851183  JORFVERS000046851183       0        NaN   \n",
      "454297  JORFTEXT000046851183  JORFARTI000046851184       1        NaN   \n",
      "454298  JORFTEXT000046851186                   NaN       0        NaN   \n",
      "454299  JORFTEXT000046851186  JORFVERS000046851186       0        NaN   \n",
      "454300  JORFTEXT000046851186  JORFARTI000046851187       1        NaN   \n",
      "\n",
      "        N° alinéa                                            Contenu  \n",
      "0               0                     fr/lr/loi/2023-1380/2023-12-31  \n",
      "1               0  LOI n° 2023-1380 du 30 décembre 2023 visant à ...  \n",
      "2               1  I. — Après l'article L. 2122-19 du code généra...  \n",
      "3               2  « Art. L. 2122-19-1. — Pour assurer les foncti...  \n",
      "4               3  II. — L'article L. 2122-19-1 du code général d...  \n",
      "...           ...                                                ...  \n",
      "454296          0  Arrêté du 30 décembre 2022 portant admission à...  \n",
      "454297          1  Par arrêté du garde des sceaux, ministre de la...  \n",
      "454298          0               fr/lr/arrete/TREL2234954A/2023-01-01  \n",
      "454299          0  Arrêté du 22 décembre 2022 portant nomination ...  \n",
      "454300          1  Par arrêté du ministre de la transition écolog...  \n",
      "\n",
      "[454301 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Spécifiez le chemin vers votre fichier CSV\n",
    "csv_path = '../jorf_2023.csv'\n",
    "\n",
    "# Utilisez Pandas pour lire le fichier CSV dans un DataFrame\n",
    "df = pd.read_csv(csv_path, sep='|',names=[\"ID texte\",\"ID article\",\"Nature\",\"N° article\",\"N° alinéa\",\"Contenu\"])\n",
    "\n",
    "# Affichez le DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract from a csv file the text to be tokenized\n",
    "def extract_text_from_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        text = f.readlines()\n",
    "    return text\n",
    "\n",
    "\n",
    "# Extract from the text what is between the first « and »\n",
    "def extract_text_inside(text):\n",
    "    match_inside = re.search(r'«(.+)', text)\n",
    "    text_inside = None\n",
    "    if match_inside:\n",
    "        text_inside = match_inside.group(1)\n",
    "    return text_inside\n",
    "\n",
    "# Extract from the text what is outside the first « and »\n",
    "def extract_text_outside(text):\n",
    "    match_outside = re.search(r'^[^«]+', text)\n",
    "    text_outside = None\n",
    "    if match_outside:\n",
    "        text_outside = match_outside.group(0)\n",
    "    return text_outside\n",
    "\n",
    "def extracted_data_to_text_file(df, fun1, fun2):\n",
    "    df['Law'] = df['Contenu'].apply(fun1)\n",
    "    df['Not_Law'] = df['Contenu'].apply(fun2)\n",
    "    \n",
    "    with open(\"../jorf_2023.txt\",\"w\") as f:\n",
    "        f.writelines(\"\\n\".join(df[\"Contenu\"].to_list()))\n",
    "    \n",
    "    # Writing what pertains to the Law\n",
    "    with open(\"../jorf_2023_Law.txt\",\"w\") as f:\n",
    "        data = \"\\n\".join(df['Law'].dropna().to_list())\n",
    "        f.writelines(data)\n",
    "\n",
    "    # Writing what DO NOT pertains to the Law\n",
    "    with open(\"../jorf_2023_Not_Law.txt\",\"w\") as f:\n",
    "        data = \"\\n\".join(df['Not_Law'].dropna().to_list())\n",
    "        f.writelines(data)\n",
    "\n",
    "\n",
    "# Write the text in a file\n",
    "def write_text(text, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        for line in text:\n",
    "            f.write(line + '\\n')\n",
    "\n",
    "# Tokenize the text with the sentencepiece model\n",
    "def tokenize_text(text, sp):\n",
    "    tokenized_text = [sp.EncodeAsPieces(line) for line in text]\n",
    "    return tokenized_text\n",
    "\n",
    "\n",
    "# Write the tokenized text in a csv file\n",
    "def write_tokenized_text(tokenized_text, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        for line in tokenized_text:\n",
    "            f.write(' '.join(line) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data_to_text_file(df, extract_text_inside, extract_text_outside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General tokenization model\n",
    "for n in (100,1000,10000):\n",
    "    spm.SentencePieceTrainer.train(input='../jorf_2023.txt', model_prefix=f'../models/{n}_tokens', vocab_size=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100 is not enough while 10000 is excessive.\n",
    "We will stay at a 1000 for the specialized models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specialized in Law tokenization model\n",
    "spm.SentencePieceTrainer.train(input='../jorf_2023_Law.txt', model_prefix=f'../models/1000_tokens_Law', vocab_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specialized in Not_Law tokenization model\n",
    "spm.SentencePieceTrainer.train(input='../jorf_2023_Not_Law.txt', model_prefix=f'../models/1000_tokens_Not_Law', vocab_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text from the text files to a csv file\n",
    "sp_general = spm.SentencePieceProcessor()\n",
    "sp_general.Load('../models/1000_tokens.model')\n",
    "sp_law = spm.SentencePieceProcessor()\n",
    "sp_law.Load('../models/1000_tokens_Law.model')\n",
    "sp_not_law = spm.SentencePieceProcessor()\n",
    "sp_not_law.Load('../models/1000_tokens_Not_Law.model')\n",
    "\n",
    "general_tokenized = tokenize_text(extract_text_from_file('../jorf_2023.txt'), sp_general)\n",
    "law_tokenized = tokenize_text(extract_text_from_file('../jorf_2023_Law.txt'), sp_law)\n",
    "not_law_tokenized = tokenize_text(extract_text_from_file('../jorf_2023_Not_Law.txt'), sp_not_law)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_general_tokenized = '../jorf_2023_general_tokenized.csv'\n",
    "output_file_law_tokenized = '../jorf_2023_law_tokenized.csv'\n",
    "output_file_not_law_tokenized = '../jorf_2023_not_law_tokenized.csv'\n",
    "\n",
    "write_tokenized_text(general_tokenized, output_file_general_tokenized)\n",
    "write_tokenized_text(law_tokenized, output_file_law_tokenized)\n",
    "write_tokenized_text(not_law_tokenized, output_file_not_law_tokenized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "juritok",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
